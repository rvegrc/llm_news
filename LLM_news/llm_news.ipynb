{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c746721e924b3b8a39882e1b3e798a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8376f7582b7b4585aad5413e4b193fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"../models/Mistral-7B-Instruct-v0.3\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50},\n",
    ")\n",
    "\n",
    "\n",
    "cpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"../models/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step.\n",
      "\n",
      "Electroencephalography (EEG) is a technique used to record the electrical activity of the brain.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1. 'Electro': Related to electricity.\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "gpu_chain = prompt | gpu_llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(gpu_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step. Electroencephalography, or EEG, is a method used to record electrical activity in the brain. It works by placing small sensors, called electrodes, on the scalp. These electrodes detect the electrical signals produced by the brain's neurons as they communicate\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "cpu_chain = prompt | cpu_llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(cpu_chain.invoke({\"question\": question}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
